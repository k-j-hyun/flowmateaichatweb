import cv2
import numpy as np
import os

# MediaPipe import with fallback to OpenCV
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("MediaPipeë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
except ImportError as e:
    print(f"MediaPipeë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("OpenCV fallbackì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    MEDIAPIPE_AVAILABLE = False

def analyze_visual_features(video_path: str, frame_skip: int = 5, resize_dim=(640, 360)) -> dict:
    cap = cv2.VideoCapture(video_path)

    if MEDIAPIPE_AVAILABLE:
        return _analyze_with_mediapipe(cap, frame_skip, resize_dim)
    else:
        return _analyze_with_opencv(cap, frame_skip, resize_dim)

def _analyze_with_mediapipe(cap, frame_skip: int, resize_dim: tuple) -> dict:
    """MediaPipeë¥¼ ì‚¬ìš©í•œ ë¶„ì„"""
    # MediaPipe ì´ˆê¸°í™”
    mp_face_detection = mp.solutions.face_detection
    mp_pose = mp.solutions.pose
    mp_hands = mp.solutions.hands

    # MediaPipe ëª¨ë¸ ì´ˆê¸°í™”
    face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)
    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True, min_detection_confidence=0.5)
    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)

    frame_count = 0
    analyzed_frames = 0
    face_detected = 0
    pose_detected = 0
    hand_gesture_detected = 0
    
    # ì œìŠ¤ì²˜ ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    previous_landmarks = None
    movement_threshold = 0.05

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if frame_count % frame_skip != 0:
            continue

        frame_resized = cv2.resize(frame, resize_dim)
        rgb_frame = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)

        # ì–¼êµ´ ê²€ì¶œ
        face_results = face_detection.process(rgb_frame)
        if face_results.detections:
            face_detected += 1

        # í¬ì¦ˆ ê²€ì¶œ
        pose_results = pose.process(rgb_frame)
        current_pose_detected = False
        if pose_results.pose_landmarks:
            pose_detected += 1
            current_pose_detected = True
            
            # í¬ì¦ˆ ëœë“œë§ˆí¬ë¥¼ ì´ìš©í•œ ì›€ì§ì„ ë¶„ì„
            current_landmarks = []
            for landmark in pose_results.pose_landmarks.landmark:
                current_landmarks.append([landmark.x, landmark.y])
            
            if previous_landmarks is not None:
                movement = np.array(current_landmarks) - np.array(previous_landmarks)
                avg_movement = np.mean(np.abs(movement))
                if avg_movement > movement_threshold:
                    hand_gesture_detected += 1
            
            previous_landmarks = current_landmarks

        # ì† ì œìŠ¤ì²˜ ê²€ì¶œ
        hand_results = hands.process(rgb_frame)
        if hand_results.multi_hand_landmarks:
            if not current_pose_detected:
                hand_gesture_detected += 1

        analyzed_frames += 1

    cap.release()
    face_detection.close()
    pose.close() 
    hands.close()

    return {
        "total_frames": analyzed_frames,
        "face_detected_frames": face_detected,
        "pose_detected_frames": pose_detected,
        "gesture_detected_frames": hand_gesture_detected,
        "face_detection_ratio": round(face_detected / analyzed_frames, 2) if analyzed_frames else 0.0,
        "pose_detection_ratio": round(pose_detected / analyzed_frames, 2) if analyzed_frames else 0.0,
        "gesture_ratio": round(hand_gesture_detected / analyzed_frames, 2) if analyzed_frames else 0.0,
    }

def _analyze_with_opencv(cap, frame_skip: int, resize_dim: tuple) -> dict:
    """OpenCVë¥¼ ì‚¬ìš©í•œ fallback ë¶„ì„"""
    # OpenCV ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    body_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_upperbody.xml')

    frame_count = 0
    analyzed_frames = 0
    face_detected = 0
    gesture_detected = 0
    prev_gray = None

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if frame_count % frame_skip != 0:
            continue

        frame_resized = cv2.resize(frame, resize_dim)
        gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)

        # ì–¼êµ´ ê²€ì¶œ
        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
        if len(faces) > 0:
            face_detected += 1

        # ìƒì²´/ì›€ì§ì„ ê²€ì¶œ
        bodies = body_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3, minSize=(50, 50))
        
        # ì›€ì§ì„ ê°ì§€
        motion_detected = False
        if prev_gray is not None:
            diff = cv2.absdiff(prev_gray, gray)
            _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)
            motion_pixels = cv2.countNonZero(thresh)
            
            if motion_pixels > (resize_dim[0] * resize_dim[1] * 0.05):
                motion_detected = True
        
        if len(bodies) > 0 or motion_detected:
            gesture_detected += 1

        prev_gray = gray.copy()
        analyzed_frames += 1

    cap.release()

    return {
        "total_frames": analyzed_frames,
        "face_detected_frames": face_detected,
        "pose_detected_frames": 0,  # OpenCVì—ì„œëŠ” í¬ì¦ˆ ê²€ì¶œ ë¶ˆê°€
        "gesture_detected_frames": gesture_detected,
        "face_detection_ratio": round(face_detected / analyzed_frames, 2) if analyzed_frames else 0.0,
        "pose_detection_ratio": 0.0,  # OpenCVì—ì„œëŠ” í¬ì¦ˆ ê²€ì¶œ ë¶ˆê°€
        "gesture_ratio": round(gesture_detected / analyzed_frames, 2) if analyzed_frames else 0.0,
    }

# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    import sys
    import os

    default_video = "uploads/sample_video.mp4"
    video_path = sys.argv[1] if len(sys.argv) > 1 else default_video

    if not os.path.exists(video_path):
        print(f"ì˜ìƒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {video_path}")
    else:
        print(f"ë¶„ì„ ì¤‘ì¸ ì˜ìƒ íŒŒì¼: {video_path}\n")
        result = analyze_visual_features(video_path)

        print("ğŸ¥ ì‹œê° í‘œí˜„ ë¶„ì„ ê²°ê³¼:")
        for k, v in result.items():
            print(f"- {k}: {v}")
