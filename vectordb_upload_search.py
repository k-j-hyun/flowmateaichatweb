import os
import hashlib
import re
from parsing_utils import split_chunks

# Qdrant import ì‹œë„
try:
    from langchain_qdrant import Qdrant
    QDRANT_AVAILABLE = True
except ImportError:
    try:
        from langchain_community.vectorstores import Qdrant
        QDRANT_AVAILABLE = True
    except ImportError:
        QDRANT_AVAILABLE = False

from langchain_ollama import OllamaEmbeddings, ChatOllama
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from collections import deque

# ì „ì—­ ìºì‹œ
_vector_store_cache = {}
_client_cache = None

class BufferMemory:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬"""
    def __init__(self, max_turns=5):
        self.max_turns = max_turns
        self.history = deque(maxlen=max_turns)
    
    def append(self, user, assistant):
        self.history.append({"user": user, "assistant": assistant})
    
    def get_formatted_history(self):
        if not self.history:
            return ""
        return "\n".join([f"User: {h['user']}\nAssistant: {h['assistant']}" for h in self.history])

def get_file_hash(file_path: str) -> str:
    """íŒŒì¼ í•´ì‹œ ìƒì„± (í¬ê¸°+ìˆ˜ì •ì‹œê°„ ê¸°ë°˜)"""
    try:
        stat = os.stat(file_path)
        quick_hash = f"{stat.st_size}_{int(stat.st_mtime)}"
        return hashlib.md5(quick_hash.encode()).hexdigest()
    except:
        return hashlib.md5(file_path.encode()).hexdigest()

def get_qdrant_client():
    """Qdrant í´ë¼ì´ì–¸íŠ¸ ìºì‹±"""
    global _client_cache
    if _client_cache is None:
        try:
            _client_cache = QdrantClient(host="localhost", port=6333)
        except Exception as e:
            print(f"[Qdrant ì—°ê²° ì‹¤íŒ¨: {e}]")
            return None
    return _client_cache

def get_llm(tokens=256):
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í˜¸í™˜ì„± ì²˜ë¦¬)"""
    model_name = "anpigon/qwen2.5-7b-instruct-kowiki:latest"
    # model_name = "exaone3.5:latest"
    try:
        return ChatOllama(model=model_name, temperature=0.2, num_predict=tokens)
    except TypeError:
        try:
            return ChatOllama(model=model_name, temperature=0.2, max_tokens=tokens)
        except TypeError:
            return ChatOllama(model=model_name, temperature=0.2)

def data_to_vectorstore(file_path: str):
    """ë²¡í„°ìŠ¤í† ì–´ - ìºì‹± ë° ë¹ ë¥¸ ì²´í¬"""
    
    # ìºì‹œ í™•ì¸ (ê°€ì¥ ë¹ ë¥¸ ê²½ë¡œ)
    file_hash = get_file_hash(file_path)
    cache_key = f"{file_path}_{file_hash}"
    
    if cache_key in _vector_store_cache:
        print(f"[ìºì‹œì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ: {file_path}]")
        return _vector_store_cache[cache_key]
    
    if not QDRANT_AVAILABLE:
        print("[Qdrant ì‚¬ìš© ë¶ˆê°€ - None ë°˜í™˜]")
        return None
    
    client = get_qdrant_client()
    if client is None:
        return None
    
    collection_name = f"doc_{file_hash}"
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¹ ë¥¸ í™•ì¸
    try:
        existing_collections = [col.name for col in client.get_collections().collections]
        
        if collection_name in existing_collections:
            print(f"[ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©: {collection_name}]")
            
            # ë²¡í„° ìˆ˜ ë¹ ë¥¸ ì²´í¬
            try:
                collection_info = client.get_collection(collection_name)
                print(collection_info)
                if collection_info.points_count  > 0:
                    vector_store = Qdrant(
                        client=client,
                        collection_name=collection_name,
                        embeddings=OllamaEmbeddings(model="bona/bge-m3-korean:latest")
                    )
                    
                    # ìºì‹œì— ì €ì¥
                    _vector_store_cache[cache_key] = vector_store
                    return vector_store
                else:
                    print("[ë¹ˆ ì»¬ë ‰ì…˜ ê°ì§€ - ì‚­ì œ]")
                    client.delete_collection(collection_name)
            except:
                print("[ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ - ì‚­ì œ í›„ ì¬ìƒì„±]")
                try:
                    client.delete_collection(collection_name)
                except:
                    pass
    except:
        print("[ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨]")
    
    # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (í•„ìš”í•œ ê²½ìš°ë§Œ)
    print(f"[ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}]")
    
    try:
        # ë¬¸ì„œ ì²­í‚¹ - ê¸°ì¡´ê³¼ ë™ì¼
        documents = split_chunks(file_path)
        if not documents:
            return None
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
        
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€
        vector_store = Qdrant(
            client=client,
            collection_name=collection_name,
            embeddings=OllamaEmbeddings(model="bona/bge-m3-korean")
        )
        
        print("ì„ë² ë”© ë° ì €ì¥ ì¤‘...")
        ids = [doc.metadata['order'] for doc in documents]
        vector_store.add_documents(documents, ids=ids)
        
        # ìºì‹œì— ì €ì¥
        _vector_store_cache[cache_key] = vector_store
        print(f"[ë²¡í„°ìŠ¤í† ì–´ ìºì‹± ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ]")
        
        return vector_store
        
    except Exception as e:
        print(f"[ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}]")
        return None

def smart_determine_params(query: str):
    """ê°œì„ ëœ íŒŒë¼ë¯¸í„° ê²°ì • - ë‹µë³€ í’ˆì§ˆ ê³ ë ¤"""

    
    # ë³µì¡í•œ ì‘ì—… (ë” ë§ì€ í† í°ê³¼ ë¬¸ì„œ í•„ìš”)
    if any(keyword in query for keyword in ['ë³´ê³ ì„œ', 'ë°œí‘œ', 'ppt', 'ë¶„ì„', 'ë¹„êµ', 'í‰ê°€']):
        return 1000, 4096, "ë³µí•©ë¶„ì„"
    
    # í€´ì¦ˆ/ë¬¸ì œ (ì ë‹¹í•œ ì–‘ì˜ ë¬¸ì„œ, êµ¬ì¡°í™”ëœ ë‹µë³€)
    elif any(keyword in query for keyword in ['í€´ì¦ˆ']):
        return 100, 2048, "í€´ì¦ˆ"
    
    # ìš”ì•½ (ì „ì²´ì ì¸ ì´í•´ í•„ìš”)
    elif any(keyword in query for keyword in ['ìš”ì•½', 'ì •ë¦¬', 'í•µì‹¬', 'ê°„ì¶”']):
        return 100, 1024, "ìš”ì•½"
    
    # êµ¬ì²´ì  ì§ˆë¬¸ (ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ í•„ìš”)
    elif any(keyword in query for keyword in ['ì–´ë–»ê²Œ', 'ì™œ', 'ë¬´ì—‡', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€']):
        return 100, 2048, "êµ¬ì²´ì ì§ˆë¬¸"
    
    # ì¼ë°˜ ì§ˆë¬¸
    else:
        return 10, 1024, "ì¼ë°˜"

def create_enhanced_prompt(query: str, combined_text: str, history: str, task_type: str):
    """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    base_context = f"""
[LANGUAGE INSTRUCTION - MANDATORY]
**ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ì…ë‹ˆë‹¤.**
**ONLY Korean language allowed. Chinese/English/Japanese strictly forbidden.**
**åªèƒ½ç”¨éŸ©è¯­å›ç­”ï¼Œä¸¥ç¦ä½¿ç”¨ä¸­æ–‡æˆ–å…¶ä»–è¯­è¨€ã€‚**

- ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”
- ìš”ì•½, ë³´ê³ ì„œ ì‘ì„±, ë°œí‘œìë£Œ ì‘ì„±ì— íŠ¹í™”ë˜ì–´ìˆìŠµë‹ˆë‹¤
- ì¤‘êµ­ì–´ë‚˜ ì˜ì–´ê°€ í¬í•¨ëœ ë‹µë³€ì€ ì ˆëŒ€ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”

ì´ì „ ëŒ€í™” ê¸°ë¡:
{history}

ì°¸ê³  ë¬¸ì„œ ë‚´ìš©:
{combined_text}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

**ë‹¤ì‹œ í•œ ë²ˆ ê°•ì¡°: ë‹µë³€ì€ 100% í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.**
"""

    if task_type == "ë³µí•©ë¶„ì„":
        return f"""{base_context}

**í•œêµ­ì–´ë¡œë§Œ ë‹µë³€ í•„ìˆ˜**
ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ ì²´ê³„ì ì´ê³  ì „ë¬¸ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤
- ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì„¸ìš”
- ë…¼ë¦¬ì  êµ¬ì¡°ë¡œ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- ì¤‘êµ­ì–´/ì˜ì–´ ì‚¬ìš© ì ˆëŒ€ ê¸ˆì§€

í•œêµ­ì–´ ë‹µë³€:"""

    elif task_type == "í€´ì¦ˆ":
        return f"""{base_context}

**í•œêµ­ì–´ í€´ì¦ˆ ìƒì„±**
ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ë¡œë§Œ í€´ì¦ˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
- ë¬¸ì„œì˜ í•µì‹¬ ê°œë…ê³¼ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”
- ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë¬¸ì œë¥¼ í¬í•¨í•˜ì„¸ìš” (ê°ê´€ì‹, ë‹¨ë‹µí˜•, ì„œìˆ í˜• ë“±)
- ì‚¬ìš©ìì˜ ìš”ì²­ì´ ì—†ë‹¤ë©´ ë¬¸ì œëŠ” 5ê°œë§Œ ìƒì„±í•©ë‹ˆë‹¤
- ê° ë¬¸ì œì— ëŒ€í•œ ì •ë‹µê³¼ í•´ì„¤ì„ ì œê³µí•˜ì„¸ìš”
- ë‚œì´ë„ë¥¼ ì ì ˆíˆ ì¡°ì ˆí•˜ì„¸ìš”
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”

í•œêµ­ì–´ í€´ì¦ˆ:"""

    elif task_type == "ìš”ì•½":
        return f"""{base_context}

**í•œêµ­ì–´ ìš”ì•½**
ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ í•œêµ­ì–´ë¡œë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”:
- í•µì‹¬ ì£¼ì œì™€ ìš”ì ì„ ëª…í™•íˆ ì •ë¦¬í•˜ì„¸ìš”
- ì¤‘ìš”ë„ì— ë”°ë¼ ë‚´ìš©ì„ êµ¬ì¡°í™”í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ë°ì´í„°ë‚˜ ì˜ˆì‹œê°€ ìˆë‹¤ë©´ í¬í•¨í•˜ì„¸ìš”
- ê°„ê²°í•˜ì§€ë§Œ í¬ê´„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”

í•œêµ­ì–´ ìš”ì•½:"""

    elif task_type == "êµ¬ì²´ì ì§ˆë¬¸":
        return f"""{base_context}

**í•œêµ­ì–´ë¡œ êµ¬ì²´ì  ë‹µë³€**
ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ êµ¬ì²´ì ì´ê³  ì •í™•í•˜ê²Œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ë¬¸ì„œì—ì„œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì•„ ê·¼ê±°ë¡œ ì œì‹œí•˜ì„¸ìš”
- ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
- ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë¶€ë¶„ì€ "ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
- ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš”
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”

í•œêµ­ì–´ ë‹µë³€:"""

    else:  # ì¼ë°˜
        return f"""{base_context}

**í•œêµ­ì–´ë¡œ ì¼ë°˜ ë‹µë³€**
ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:
- ë¬¸ì„œì˜ ê´€ë ¨ ë‚´ìš©ì„ ì¶©ë¶„íˆ í™œìš©í•˜ì„¸ìš”
- ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
- ì¶”ê°€ì ì¸ ë§¥ë½ì´ë‚˜ ë°°ê²½ ì •ë³´ë„ ì œê³µí•˜ì„¸ìš”
- ë¬¸ì„œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¶”ì¸¡ì€ í”¼í•˜ì„¸ìš”
- ë‹µë³€ì€ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ í•´ì£¼ì„¸ìš”
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”

í•œêµ­ì–´ ë‹µë³€:"""

def question_answer_with_memory(file_path: str, query: str, memory: BufferMemory, tokens=256) -> str:
    """ê°œì„ ëœ ë©”ì¸ í•¨ìˆ˜ - ë‹µë³€ í’ˆì§ˆê³¼ ì„±ëŠ¥ ê· í˜•"""
    
    # 1. í–¥ìƒëœ íŒŒë¼ë¯¸í„° ê²°ì •
    k, optimized_tokens, task_type = smart_determine_params(query)
    final_tokens = max(tokens, optimized_tokens) if tokens != 256 else optimized_tokens
    
    print(f"[ì‘ì—… ìœ í˜•: {task_type}, ë¬¸ì„œ ìˆ˜: {k}, í† í°: {final_tokens}]")
    
    # 2. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (ìºì‹±ë¨)
    vector_store = data_to_vectorstore(file_path)
    
    # 3. ë²¡í„°ìŠ¤í† ì–´ ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ í´ë°±
    if vector_store is None:
        print("[ë²¡í„°ìŠ¤í† ì–´ ì—†ìŒ - ì§ì ‘ íŒŒì¼ ì½ê¸°]")
        return handle_fallback_mode(file_path, query, memory, final_tokens, task_type)
    
    # 4. í–¥ìƒëœ ë²¡í„° ê²€ìƒ‰
    try:
        docs = vector_store.similarity_search(query, k=k)
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¶”ê°€ ê²€ìƒ‰
        if len(docs) < k//2:
            # ì¿¼ë¦¬ë¥¼ ë‹¨ìˆœí™”í•´ì„œ ë‹¤ì‹œ ê²€ìƒ‰
            simple_query = " ".join(query.split()[:3])  # ì²˜ìŒ 3ë‹¨ì–´ë§Œ
            additional_docs = vector_store.similarity_search(simple_query, k=k)
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ í•©ì¹˜ê¸°
            seen = set()
            all_docs = []
            for doc in docs + additional_docs:
                doc_hash = hash(doc.page_content[:100])  # ì²« 100ìë¡œ ì¤‘ë³µ íŒë‹¨
                if doc_hash not in seen:
                    seen.add(doc_hash)
                    all_docs.append(doc)
                if len(all_docs) >= k:
                    break
            docs = all_docs
        
        combined_text = "\n\n".join([doc.page_content for doc in docs])
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì€ ê²½ìš° ì¶”ê°€ ë¬¸ì„œ ê²€ìƒ‰
        if len(combined_text) < 500:
            extra_docs = vector_store.similarity_search("", k=5)  # ì¼ë°˜ì ì¸ ë¬¸ì„œë“¤
            for doc in extra_docs:
                if doc not in docs:
                    docs.append(doc)
                    combined_text += "\n\n" + doc.page_content
                if len(combined_text) > 1000:
                    break
        
    except Exception as e:
        print(f"[ê²€ìƒ‰ ì‹¤íŒ¨: {e}] - í´ë°± ëª¨ë“œ")
        return handle_fallback_mode(file_path, query, memory, final_tokens, task_type)
    
    # 5. í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ë¡œ LLM í˜¸ì¶œ
    history = memory.get_formatted_history()
    prompt = create_enhanced_prompt(query, combined_text, history, task_type)
    
    # 6. LLM í˜¸ì¶œ
    try:
        llm = get_llm(final_tokens)
        answer = llm.invoke(prompt).content
        
        # í•œêµ­ì–´ ì‘ë‹µ í™•ì¸ ë° ì²˜ë¦¬
        answer = ensure_korean_only(answer)
        
        # ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸
        memory.append(query, answer)
        return answer
        
    except Exception as e:
        print(f"[LLM ì‹¤íŒ¨: {e}] - í´ë°± ëª¨ë“œ")
        return handle_fallback_mode(file_path, query, memory, final_tokens, task_type)

def handle_fallback_mode(file_path: str, query: str, memory: BufferMemory, tokens: int, task_type: str = "ì¼ë°˜") -> str:
    """ê°œì„ ëœ í´ë°± ëª¨ë“œ"""
    
    try:
        # íŒŒì¼ í¬ê¸°ì— ë”°ë¼ ì½ì„ ì–‘ ì¡°ì ˆ
        file_size = os.path.getsize(file_path)
        
        if file_size > 50000:  # 50KB ì´ìƒ
            read_size = 15000  # 15KBë§Œ ì½ê¸°
        elif file_size > 20000:  # 20KB ì´ìƒ
            read_size = 10000  # 10KBë§Œ ì½ê¸°
        else:
            read_size = file_size  # ì „ì²´ ì½ê¸°
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read(read_size)
        
        history = memory.get_formatted_history()
        
        # í´ë°± ëª¨ë“œì—ì„œë„ í•œêµ­ì–´ ê°•ì œ í”„ë¡¬í”„íŠ¸ ì ìš©
        prompt = f"""
[LANGUAGE INSTRUCTION - MANDATORY]
**ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¤‘êµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ì…ë‹ˆë‹¤.**

ë‹¹ì‹ ì€ FlowíŒ€ì—ì„œ ë§Œë“  FlowMate:ì‚¬ë‚´ì—…ë¬´ê¸¸ë¼ì¡ì´ AIì…ë‹ˆë‹¤. 
ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì´ì „ ëŒ€í™” ê¸°ë¡:
{history}

ì°¸ê³  ë¬¸ì„œ ë‚´ìš©:
{content}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

**í•œêµ­ì–´ë¡œë§Œ ë‹µë³€:**
"""
        
        # í† í° ìˆ˜ ì¡°ì ˆ (í´ë°± ëª¨ë“œì—ì„œëŠ” ì•½ê°„ ì¤„ì„)
        fallback_tokens = min(tokens, 2048)
        
        llm = get_llm(fallback_tokens)
        answer = llm.invoke(prompt).content
        
        # í´ë°± ëª¨ë“œì—ì„œë„ í•œêµ­ì–´ ì‘ë‹µ í™•ì¸
        answer = ensure_korean_only(answer)
        
        memory.append(query, answer)
        return answer
        
    except Exception as e:
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\në‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜ ë¬¸ì„œ í˜•ì‹ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."

def translate_to_korean(text: str) -> str:
    """ì¤‘êµ­ì–´ë‚˜ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    try:
        llm = ChatOllama(
            model="qwen2.5:7b",
            temperature=0.1,
            timeout=30.0
        )
        
        # ë²ˆì—­ ì „ìš© í”„ë¡¬í”„íŠ¸
        translation_prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

ë²ˆì—­ ê·œì¹™:
1. ì›ë¬¸ì˜ ì˜ë¯¸ì™€ ë‰˜ì•™ìŠ¤ë¥¼ ì •í™•íˆ ë³´ì¡´í•  ê²ƒ
2. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•  ê²ƒ
3. ì „ë¬¸ ìš©ì–´ëŠ” ì ì ˆí•œ í•œêµ­ì–´ ìš©ì–´ë¡œ ë²ˆì—­í•  ê²ƒ
4. ë¬¸ë‹¨ êµ¬ì¡°ì™€ ì„œì‹ì„ ìœ ì§€í•  ê²ƒ
5. ë²ˆì—­ë¬¸ë§Œ ì¶œë ¥í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ ê²ƒ

ë²ˆì—­í•  í…ìŠ¤íŠ¸:
{text}

í•œêµ­ì–´ ë²ˆì—­:"""

        response = llm.invoke(translation_prompt)
        
        if response and hasattr(response, 'content'):
            translated = response.content.strip()
            
            # ë²ˆì—­ ê²°ê³¼ê°€ ìœ íš¨í•œì§€ í™•ì¸ (í•œêµ­ì–´ í¬í•¨ ì—¬ë¶€)
            korean_chars = len(re.findall(r'[ê°€-í£]', translated))
            if korean_chars > 0:
                return translated
        
        return None
        
    except Exception as e:
        print(f"[ë²ˆì—­ ì˜¤ë¥˜] {str(e)}")
        return None

def ensure_korean_only(text: str) -> str:
    """ì¤‘êµ­ì–´ ì¤‘ì‹¬ì˜ ì‘ë‹µì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì—¬ ë°˜í™˜"""
    if not text or not isinstance(text, str):
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # í•œêµ­ì–´ ë¬¸ì ë¹„ìœ¨ ì²´í¬
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(re.sub(r'[\s\n\r\t\.,;:!?\-\(\)\[\]{}\"\'`~@#$%^&*+=|\\/<>]', '', text))
    
    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ í†µê³¼
    if total_chars < 10:
        return text
    
    # í•œêµ­ì–´ê°€ ì „í˜€ ì—†ê³  ì¤‘êµ­ì–´ê°€ ë§ì€ ê²½ìš° ë²ˆì—­ ì‹œë„
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff\u3400-\u4dbf\uf900-\ufaff]', text))
    
    # ì¤‘êµ­ì–´ ì‘ë‹µ ê°ì§€: í•œêµ­ì–´ê°€ 10% ë¯¸ë§Œì´ë©´ì„œ ì¤‘êµ­ì–´ê°€ 30% ì´ìƒì¸ ê²½ìš°
    if korean_chars < total_chars * 0.1 and chinese_chars > total_chars * 0.3:
        print("[ì–¸ì–´ ê°ì§€] ì¤‘êµ­ì–´ ì‘ë‹µ ê°ì§€ë¨, í•œêµ­ì–´ë¡œ ë²ˆì—­ ì‹œë„ ì¤‘...")
        translated = translate_to_korean(text)
        if translated:
            return f"{translated}\n\nğŸ’¡ ì›ë³¸ì´ ì¤‘êµ­ì–´ë¡œ ìƒì„±ë˜ì–´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì˜€ìŠµë‹ˆë‹¤."
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    
    # ì™„ì „ ì˜ì–´ ì‘ë‹µ ê°ì§€: í•œêµ­ì–´ê°€ 5% ë¯¸ë§Œì´ë©´ì„œ ì—°ì†ëœ ì˜ì–´ ë¬¸ì¥ì´ ë§ì€ ê²½ìš°
    english_sentences = re.findall(r'\b[A-Za-z]+(?:\s+[A-Za-z]+){4,}\b', text)
    if korean_chars < total_chars * 0.05 and len(' '.join(english_sentences)) > total_chars * 0.5:
        print("[ì–¸ì–´ ê°ì§€] ì˜ì–´ ì‘ë‹µ ê°ì§€ë¨, í•œêµ­ì–´ë¡œ ë²ˆì—­ ì‹œë„ ì¤‘...")
        translated = translate_to_korean(text)
        if translated:
            return f"{translated}\n\nğŸ’¡ ì›ë³¸ì´ ì˜ì–´ë¡œ ìƒì„±ë˜ì–´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì˜€ìŠµë‹ˆë‹¤."
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    
    # ì •ìƒì ì¸ í•œêµ­ì–´ ì‘ë‹µì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return text

def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    global _vector_store_cache, _client_cache
    _vector_store_cache.clear()
    _client_cache = None
    print("[ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ]")

def get_cache_stats():
    """ìºì‹œ ìƒíƒœ í™•ì¸"""
    return {
        "vector_stores": len(_vector_store_cache),
        "client_connected": _client_cache is not None
    }

# Django í˜¸í™˜ì„± ìœ ì§€
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    memory = BufferMemory()
    
    print("=== ê°œì„ ëœ ë²„ì „ í…ŒìŠ¤íŠ¸ ===")
    
    import time
    start_time = time.time()
    result1 = question_answer_with_memory("temp/sample.txt", "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?", memory)
    print(f"ì²« ë²ˆì§¸ ë‹µë³€: {result1[:200]}...")
    
    start_time = time.time()
    result2 = question_answer_with_memory("temp/sample.txt", "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê¸°ìˆ ì´ ì‚¬ìš©ë˜ì—ˆë‚˜ìš”?", memory)
    print(f"ë‘ ë²ˆì§¸ ë‹µë³€: {result2[:200]}...")
    
    print(f"\nìºì‹œ ìƒíƒœ: {get_cache_stats()}")